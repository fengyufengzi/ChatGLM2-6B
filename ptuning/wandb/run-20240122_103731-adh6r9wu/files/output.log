
01/22/2024 10:37:37 - WARNING - transformers_modules.chatglm2-6b.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1786] 2024-01-22 10:37:37,412 >> ***** Running training *****
[INFO|trainer.py:1787] 2024-01-22 10:37:37,412 >>   Num examples = 2
[INFO|trainer.py:1788] 2024-01-22 10:37:37,412 >>   Num Epochs = 3,000
[INFO|trainer.py:1789] 2024-01-22 10:37:37,412 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1790] 2024-01-22 10:37:37,412 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:1791] 2024-01-22 10:37:37,412 >>   Gradient Accumulation steps = 16
[INFO|trainer.py:1792] 2024-01-22 10:37:37,412 >>   Total optimization steps = 3,000
[INFO|trainer.py:1793] 2024-01-22 10:37:37,413 >>   Number of trainable parameters = 917,504
[INFO|integrations.py:727] 2024-01-22 10:37:37,439 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|                                                                                                    | 0/3000 [00:00<?, ?it/s]/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

  0%|▏                                                                                           | 7/3000 [00:04<29:30,  1.69it/s]



  1%|▌                                                                                          | 18/3000 [00:10<27:58,  1.78it/s]



  1%|▊                                                                                          | 28/3000 [00:16<27:52,  1.78it/s]




  1%|█▎                                                                                         | 42/3000 [00:24<27:44,  1.78it/s]


  2%|█▌                                                                                         | 50/3000 [00:28<27:40,  1.78it/s]



  2%|█▊                                                                                         | 60/3000 [00:34<27:34,  1.78it/s]


  2%|██                                                                                         | 67/3000 [00:38<27:29,  1.78it/s]



  3%|██▎                                                                                        | 77/3000 [00:44<27:23,  1.78it/s]



  3%|██▋                                                                                        | 89/3000 [00:50<27:17,  1.78it/s]



  3%|███                                                                                        | 99/3000 [00:56<27:12,  1.78it/s]


  4%|███▏                                                                                      | 107/3000 [01:00<27:07,  1.78it/s]



  4%|███▌                                                                                      | 117/3000 [01:06<27:01,  1.78it/s]



  4%|███▊                                                                                      | 128/3000 [01:12<26:55,  1.78it/s]




  5%|████▎                                                                                     | 142/3000 [01:20<26:47,  1.78it/s]



  5%|████▌                                                                                     | 153/3000 [01:26<26:42,  1.78it/s]


  5%|████▊                                                                                     | 160/3000 [01:30<26:37,  1.78it/s]



  6%|█████▏                                                                                    | 171/3000 [01:36<26:32,  1.78it/s]



  6%|█████▍                                                                                    | 181/3000 [01:42<26:26,  1.78it/s]



  6%|█████▊                                                                                    | 192/3000 [01:48<26:20,  1.78it/s]



  7%|██████                                                                                    | 203/3000 [01:54<26:13,  1.78it/s]


  7%|██████▎                                                                                   | 210/3000 [01:58<26:10,  1.78it/s]



  7%|██████▌                                                                                   | 220/3000 [02:04<26:03,  1.78it/s]



  8%|██████▉                                                                                   | 231/3000 [02:10<25:57,  1.78it/s]


  8%|███████▏                                                                                  | 238/3000 [02:14<25:53,  1.78it/s]



  8%|███████▍                                                                                  | 249/3000 [02:20<25:47,  1.78it/s]



  9%|███████▊                                                                                  | 260/3000 [02:27<25:41,  1.78it/s]


  9%|████████                                                                                  | 267/3000 [02:30<25:37,  1.78it/s]



  9%|████████▎                                                                                 | 277/3000 [02:36<25:31,  1.78it/s]



 10%|████████▋                                                                                 | 288/3000 [02:42<25:25,  1.78it/s]



 10%|████████▉                                                                                 | 299/3000 [02:48<25:19,  1.78it/s]



 10%|█████████▎                                                                                | 309/3000 [02:54<25:13,  1.78it/s]


 11%|█████████▌                                                                                | 317/3000 [02:59<25:09,  1.78it/s]



 11%|█████████▊                                                                                | 327/3000 [03:04<25:03,  1.78it/s]



 11%|██████████▏                                                                               | 338/3000 [03:10<24:57,  1.78it/s]



 12%|██████████▍                                                                               | 349/3000 [03:17<24:51,  1.78it/s]



 12%|██████████▊                                                                               | 359/3000 [03:22<24:45,  1.78it/s]


 12%|██████████▉                                                                               | 366/3000 [03:26<24:41,  1.78it/s]



 13%|███████████▎                                                                              | 377/3000 [03:32<24:36,  1.78it/s]



 13%|███████████▋                                                                              | 388/3000 [03:39<24:29,  1.78it/s]Traceback (most recent call last):
  File "/home/ubuntu/Documents/ai/ChatGLM2-6B/ptuning/main.py", line 447, in <module>
    main()
  File "/home/ubuntu/Documents/ai/ChatGLM2-6B/ptuning/main.py", line 379, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/transformers/trainer.py", line 1645, in train
    return inner_training_loop(
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/transformers/trainer.py", line 1938, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/transformers/trainer.py", line 2770, in training_step
    self.accelerator.backward(loss)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/accelerate/accelerator.py", line 1964, in backward
    loss.backward(**kwargs)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/ubuntu/Documents/ai/ChatGLM2-6B/ptuning/main.py", line 447, in <module>
    main()
  File "/home/ubuntu/Documents/ai/ChatGLM2-6B/ptuning/main.py", line 379, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/transformers/trainer.py", line 1645, in train
    return inner_training_loop(
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/transformers/trainer.py", line 1938, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/transformers/trainer.py", line 2770, in training_step
    self.accelerator.backward(loss)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/accelerate/accelerator.py", line 1964, in backward
    loss.backward(**kwargs)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt