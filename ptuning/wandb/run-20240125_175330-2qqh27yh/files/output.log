
01/25/2024 17:53:33 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/datasets/load.py:2483: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
[INFO|configuration_utils.py:667] 2024-01-25 17:53:34,962 >> loading configuration file /home/ubuntu/Documents/ai/model/chatglm2-6b/config.json
[INFO|configuration_utils.py:667] 2024-01-25 17:53:34,963 >> loading configuration file /home/ubuntu/Documents/ai/model/chatglm2-6b/config.json
[INFO|configuration_utils.py:725] 2024-01-25 17:53:34,964 >> Model config ChatGLMConfig {
  "_name_or_path": "/home/ubuntu/Documents/ai/model/chatglm2-6b",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1e-05,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_layers": 28,
  "original_rope": true,
  "pad_token_id": 0,
  "padded_vocab_size": 65024,
  "post_layer_norm": true,
  "pre_seq_len": null,
  "prefix_projection": false,
  "quantization_bit": 0,
  "rmsnorm": true,
  "seq_length": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.30.2",
  "use_cache": true,
  "vocab_size": 65024
}
[INFO|tokenization_utils_base.py:1821] 2024-01-25 17:53:34,964 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1821] 2024-01-25 17:53:34,964 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1821] 2024-01-25 17:53:34,964 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1821] 2024-01-25 17:53:34,964 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:2575] 2024-01-25 17:53:35,004 >> loading weights file /home/ubuntu/Documents/ai/model/chatglm2-6b/pytorch_model.bin.index.json
[INFO|configuration_utils.py:577] 2024-01-25 17:53:35,004 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}

Loading checkpoint shards:  57%|██████████████████████████████████████▊                             | 4/7 [00:02<00:02,  1.44it/s]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████| 7/7 [00:04<00:00,  1.51it/s]
[INFO|modeling_utils.py:3295] 2024-01-25 17:53:39,652 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.
[WARNING|modeling_utils.py:3297] 2024-01-25 17:53:39,652 >> Some weights of ChatGLMForConditionalGeneration were not initialized from the model checkpoint at /home/ubuntu/Documents/ai/model/chatglm2-6b and are newly initialized: ['transformer.prefix_encoder.embedding.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|modeling_utils.py:2927] 2024-01-25 17:53:39,653 >> Generation config file not found, using a generation config created from the model config.
prefix: 你是专业客服人员，请用最简洁的语言回答用户的问题：
    问题：湖北高考成绩会发短信吗？
    回答：不会。可自行在网上查询。
    问题：长沙在读大学生落户有补贴吗？
    回答：长沙在读大学生落户没有补贴，本科及以上大学生毕业之后落户长沙且在长沙工作符合相关条件的可以申领租房和生活补贴。
    问题：青岛市老年人高龄补贴标准是多少？
    回答：对不起，根据参考资料无法回答。
    问题：
Running tokenizer on train dataset:  35%|███████████████▎                            | 2000/5726 [00:01<00:02, 1570.90 examples/s]
input_ids [64790, 64792, 30910, 34607, 31720, 37605, 31698, 31123, 55073, 54437, 40697, 40042, 33287, 32053, 32184, 31211, 13, 13, 13, 296, 31639, 31211, 33173, 33390, 32153, 54549, 54559, 37980, 55398, 31514, 13, 296, 33287, 31211, 31777, 31155, 54568, 33275, 40679, 34262, 31155, 13, 296, 31639, 31211, 34102, 50674, 33482, 38941, 54536, 34025, 55398, 31514, 13, 296, 33287, 31211, 34102, 50674, 33482, 38941, 31631, 34025, 31123, 30910, 49685, 54638, 30954, 54855, 54539, 35679, 30946, 49685, 54638, 54855, 54973, 54714, 30939, 30970, 54943, 33599, 56005, 54747, 35679, 30946, 49685, 54638, 56005, 55901, 54714, 30943, 30939, 54943, 33217, 54750, 41189, 30954, 58812, 55555, 35679, 30946, 54750, 41189, 55148, 55555, 33136, 30939, 30939, 55067, 33599, 55344, 54615, 35679, 30946, 54750, 41189, 54809, 56214, 54704, 54641, 30939, 30943, 54943, 33217, 56446, 34582, 30954, 54547, 56365, 35679, 30946, 56446, 34582, 57591, 55086, 54662, 30973, 54943, 33599, 54645, 56890, 55284, 35679, 30946, 56446, 34582, 55066, 55771, 33856, 30966, 30978, 30973, 56519, 30939, 30943, 54943, 33217, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
inputs 你是专业客服人员，请用最简洁的语言回答用户的问题：
    问题：湖北高考成绩会发短信吗？
    回答：不会。可自行在网上查询。
    问题：长沙在读大学生落户有补贴吗？
    回答：长沙在读大学生落户没有补贴， 鼓楼区:华大派出所(鼓楼区华林路15号)、洪山派出所(鼓楼区洪甘路21号)。台江区:鳌峰派出所(台江区亚峰小区11座)、宁化派出所(台江区万寿二道12号)。仓山区:上渡派出所(仓山区堤边里8号)、三叉街派出所(仓山区则徐大道368弄12号)。
label_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 49685, 54638, 30954, 54855, 54539, 35679, 30946, 49685, 54638, 54855, 54973, 54714, 30939, 30970, 54943, 33599, 56005, 54747, 35679, 30946, 49685, 54638, 56005, 55901, 54714, 30943, 30939, 54943, 33217, 54750, 41189, 30954, 58812, 55555, 35679, 30946, 54750, 41189, 55148, 55555, 33136, 30939, 30939, 55067, 33599, 55344, 54615, 35679, 30946, 54750, 41189, 54809, 56214, 54704, 54641, 30939, 30943, 54943, 33217, 56446, 34582, 30954, 54547, 56365, 35679, 30946, 56446, 34582, 57591, 55086, 54662, 30973, 54943, 33599, 54645, 56890, 55284, 35679, 30946, 56446, 34582, 55066, 55771, 33856, 30966, 30978, 30973, 56519, 30939, 30943, 54943, 33217, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]

Running tokenizer on train dataset: 100%|████████████████████████████████████████████| 5726/5726 [00:03<00:00, 1547.22 examples/s]
01/25/2024 17:53:47 - WARNING - transformers_modules.chatglm2-6b.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[INFO|trainer.py:577] 2024-01-25 17:53:46,698 >> max_steps is given, it will override any value given in num_train_epochs
/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1786] 2024-01-25 17:53:47,160 >> ***** Running training *****
[INFO|trainer.py:1787] 2024-01-25 17:53:47,160 >>   Num examples = 5,725
[INFO|trainer.py:1788] 2024-01-25 17:53:47,160 >>   Num Epochs = 6
[INFO|trainer.py:1789] 2024-01-25 17:53:47,160 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1790] 2024-01-25 17:53:47,160 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:1791] 2024-01-25 17:53:47,160 >>   Gradient Accumulation steps = 16
[INFO|trainer.py:1792] 2024-01-25 17:53:47,160 >>   Total optimization steps = 2,100
[INFO|trainer.py:1793] 2024-01-25 17:53:47,161 >>   Number of trainable parameters = 1,835,008
[INFO|integrations.py:727] 2024-01-25 17:53:47,188 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|                                                                                                    | 0/2100 [00:00<?, ?it/s]/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(








  0%|▍                                                                                         | 9/2100 [00:41<2:37:30,  4.52s/it]










  1%|▊                                                                                        | 19/2100 [01:26<2:36:43,  4.52s/it]










  1%|█▏                                                                                       | 29/2100 [02:33<4:56:49,  8.60s/it]










  2%|█▋                                                                                       | 39/2100 [04:11<5:33:10,  9.70s/it]










  2%|██                                                                                       | 49/2100 [05:48<5:32:35,  9.73s/it]











  3%|██▌                                                                                      | 60/2100 [07:35<5:30:51,  9.73s/it]










  3%|██▉                                                                                      | 70/2100 [09:12<5:29:12,  9.73s/it]










  4%|███▍                                                                                     | 80/2100 [10:50<5:27:36,  9.73s/it]










  4%|███▊                                                                                     | 90/2100 [12:27<5:26:03,  9.73s/it]










  5%|████▏                                                                                   | 100/2100 [14:04<5:24:20,  9.73s/it]










  5%|████▌                                                                                   | 110/2100 [15:42<5:22:45,  9.73s/it]










  6%|█████                                                                                   | 120/2100 [17:19<5:21:09,  9.73s/it]










  6%|█████▍                                                                                  | 130/2100 [18:56<5:19:31,  9.73s/it]









  7%|█████▊                                                                                  | 139/2100 [20:24<5:18:01,  9.73s/it]










  7%|██████▏                                                                                 | 149/2100 [22:01<5:16:25,  9.73s/it]










  8%|██████▋                                                                                 | 159/2100 [23:38<5:14:45,  9.73s/it]










  8%|███████                                                                                 | 169/2100 [25:16<5:13:09,  9.73s/it]










  9%|███████▌                                                                                | 179/2100 [26:53<5:11:31,  9.73s/it]










  9%|███████▉                                                                                | 189/2100 [28:30<5:09:55,  9.73s/it]










  9%|████████▎                                                                               | 199/2100 [30:08<5:08:18,  9.73s/it]










 10%|████████▊                                                                               | 209/2100 [31:45<5:06:39,  9.73s/it]










 10%|█████████▏                                                                              | 219/2100 [33:22<5:05:03,  9.73s/it]









 11%|█████████▌                                                                              | 229/2100 [34:59<5:03:26,  9.73s/it]










 11%|██████████                                                                              | 239/2100 [36:37<5:01:47,  9.73s/it]










 12%|██████████▍                                                                             | 249/2100 [38:14<5:00:10,  9.73s/it]










 12%|██████████▊                                                                             | 259/2100 [39:51<4:58:27,  9.73s/it]










 13%|███████████▎                                                                            | 269/2100 [41:29<4:56:55,  9.73s/it]










 13%|███████████▋                                                                            | 279/2100 [43:06<4:55:15,  9.73s/it]










 14%|████████████                                                                            | 289/2100 [44:43<4:53:37,  9.73s/it]










 14%|████████████▌                                                                           | 299/2100 [46:20<4:52:01,  9.73s/it]










 15%|████████████▉                                                                           | 309/2100 [47:58<4:50:25,  9.73s/it]










 15%|█████████████▎                                                                          | 319/2100 [49:35<4:48:47,  9.73s/it]










 16%|█████████████▊                                                                          | 329/2100 [51:12<4:47:10,  9.73s/it]










 16%|██████████████▏                                                                         | 339/2100 [52:50<4:45:30,  9.73s/it]










 17%|██████████████▌                                                                         | 349/2100 [54:27<4:43:53,  9.73s/it]










 17%|███████████████                                                                         | 359/2100 [56:04<4:42:15,  9.73s/it]










 18%|███████████████▍                                                                        | 369/2100 [57:41<4:40:39,  9.73s/it]










 18%|███████████████▉                                                                        | 379/2100 [59:19<4:39:01,  9.73s/it]










 19%|███████████████▉                                                                      | 389/2100 [1:00:56<4:37:24,  9.73s/it]










 19%|████████████████▎                                                                     | 399/2100 [1:02:33<4:35:46,  9.73s/it]










 19%|████████████████▋                                                                     | 409/2100 [1:04:10<4:34:07,  9.73s/it]










 20%|█████████████████▏                                                                    | 419/2100 [1:05:48<4:32:33,  9.73s/it]











 20%|█████████████████▌                                                                    | 430/2100 [1:07:35<4:30:50,  9.73s/it]









 21%|█████████████████▉                                                                    | 439/2100 [1:09:02<4:29:19,  9.73s/it]










 21%|██████████████████▍                                                                   | 449/2100 [1:10:40<4:27:42,  9.73s/it]










 22%|██████████████████▊                                                                   | 459/2100 [1:12:17<4:26:04,  9.73s/it]










 22%|███████████████████▏                                                                  | 469/2100 [1:13:54<4:24:27,  9.73s/it]










 23%|███████████████████▌                                                                  | 479/2100 [1:15:31<4:22:49,  9.73s/it]










 23%|████████████████████                                                                  | 489/2100 [1:17:09<4:21:10,  9.73s/it]










 24%|████████████████████▍                                                                 | 499/2100 [1:18:46<4:19:32,  9.73s/it]










 24%|████████████████████▊                                                                 | 509/2100 [1:20:23<4:17:58,  9.73s/it]










 25%|█████████████████████▎                                                                | 519/2100 [1:22:01<4:16:20,  9.73s/it]










 25%|█████████████████████▋                                                                | 529/2100 [1:23:38<4:14:43,  9.73s/it]










 26%|██████████████████████                                                                | 539/2100 [1:25:15<4:13:04,  9.73s/it]










 26%|██████████████████████▍                                                               | 549/2100 [1:26:52<4:11:31,  9.73s/it]










 27%|██████████████████████▉                                                               | 559/2100 [1:28:30<4:09:50,  9.73s/it]










 27%|███████████████████████▎                                                              | 569/2100 [1:30:07<4:08:16,  9.73s/it]










 28%|███████████████████████▋                                                              | 579/2100 [1:31:44<4:06:36,  9.73s/it]










 28%|████████████████████████                                                              | 589/2100 [1:33:22<4:05:00,  9.73s/it]










 29%|████████████████████████▌                                                             | 599/2100 [1:34:59<4:03:23,  9.73s/it]










 29%|████████████████████████▉                                                             | 609/2100 [1:36:36<4:01:46,  9.73s/it]










 29%|█████████████████████████▎                                                            | 619/2100 [1:38:13<4:00:07,  9.73s/it]











 30%|█████████████████████████▊                                                            | 630/2100 [1:40:00<3:58:23,  9.73s/it]









 30%|██████████████████████████▏                                                           | 639/2100 [1:41:28<3:56:53,  9.73s/it]










 31%|██████████████████████████▌                                                           | 649/2100 [1:43:05<3:55:17,  9.73s/it]










 31%|██████████████████████████▉                                                           | 659/2100 [1:44:43<3:53:37,  9.73s/it]










 32%|███████████████████████████▍                                                          | 669/2100 [1:46:20<3:52:00,  9.73s/it]











 32%|███████████████████████████▊                                                          | 680/2100 [1:48:07<3:50:16,  9.73s/it]









 33%|████████████████████████████▏                                                         | 689/2100 [1:49:34<3:48:47,  9.73s/it]










 33%|████████████████████████████▋                                                         | 699/2100 [1:51:12<3:47:09,  9.73s/it]
{'loss': 3.0909, 'learning_rate': 6.666666666666667e-05, 'epoch': 1.96}
 33%|████████████████████████████▋                                                         | 700/2100 [1:51:21<3:47:03,  9.73s/it][INFO|configuration_utils.py:458] 2024-01-25 19:45:09,100 >> Configuration saved in output/adgen-chatglm2-6b-pt-128-1e-4/checkpoint-700/config.json
[INFO|configuration_utils.py:364] 2024-01-25 19:45:09,100 >> Configuration saved in output/adgen-chatglm2-6b-pt-128-1e-4/checkpoint-700/generation_config.json
[INFO|modeling_utils.py:1853] 2024-01-25 19:45:09,107 >> Model weights saved in output/adgen-chatglm2-6b-pt-128-1e-4/checkpoint-700/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-01-25 19:45:09,107 >> tokenizer config file saved in output/adgen-chatglm2-6b-pt-128-1e-4/checkpoint-700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-01-25 19:45:09,107 >> Special tokens file saved in output/adgen-chatglm2-6b-pt-128-1e-4/checkpoint-700/special_tokens_map.json
/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(








 34%|█████████████████████████████                                                         | 709/2100 [1:52:49<3:45:34,  9.73s/it]










 34%|█████████████████████████████▍                                                        | 719/2100 [1:54:26<3:43:55,  9.73s/it]










 35%|█████████████████████████████▊                                                        | 729/2100 [1:56:04<3:42:18,  9.73s/it]










 35%|██████████████████████████████▎                                                       | 739/2100 [1:57:41<3:40:42,  9.73s/it]










 36%|██████████████████████████████▋                                                       | 749/2100 [1:59:18<3:39:03,  9.73s/it]










 36%|███████████████████████████████                                                       | 759/2100 [2:00:55<3:37:24,  9.73s/it]










 37%|███████████████████████████████▍                                                      | 769/2100 [2:02:33<3:35:49,  9.73s/it]










 37%|███████████████████████████████▉                                                      | 779/2100 [2:04:10<3:34:12,  9.73s/it]










 38%|████████████████████████████████▎                                                     | 789/2100 [2:05:47<3:32:33,  9.73s/it]










 38%|████████████████████████████████▋                                                     | 799/2100 [2:07:25<3:30:56,  9.73s/it]










 39%|█████████████████████████████████▏                                                    | 809/2100 [2:09:02<3:29:19,  9.73s/it]










 39%|█████████████████████████████████▌                                                    | 819/2100 [2:10:39<3:27:41,  9.73s/it]










 39%|█████████████████████████████████▉                                                    | 829/2100 [2:12:16<3:26:04,  9.73s/it]










 40%|██████████████████████████████████▎                                                   | 839/2100 [2:13:54<3:24:27,  9.73s/it]











 40%|██████████████████████████████████▊                                                   | 850/2100 [2:15:41<3:22:38,  9.73s/it]









 41%|███████████████████████████████████▏                                                  | 859/2100 [2:17:08<3:21:12,  9.73s/it]










 41%|███████████████████████████████████▌                                                  | 869/2100 [2:18:45<3:19:35,  9.73s/it]










 42%|███████████████████████████████████▉                                                  | 879/2100 [2:20:23<3:17:57,  9.73s/it]










 42%|████████████████████████████████████▍                                                 | 889/2100 [2:22:00<3:16:20,  9.73s/it]










 43%|████████████████████████████████████▊                                                 | 899/2100 [2:23:37<3:14:44,  9.73s/it]










 43%|█████████████████████████████████████▏                                                | 909/2100 [2:25:15<3:13:05,  9.73s/it]










 44%|█████████████████████████████████████▋                                                | 919/2100 [2:26:52<3:11:30,  9.73s/it]










 44%|██████████████████████████████████████                                                | 929/2100 [2:28:29<3:09:52,  9.73s/it]










 45%|██████████████████████████████████████▍                                               | 939/2100 [2:30:06<3:08:13,  9.73s/it]










 45%|██████████████████████████████████████▊                                               | 949/2100 [2:31:44<3:06:38,  9.73s/it]










 46%|███████████████████████████████████████▎                                              | 959/2100 [2:33:21<3:04:58,  9.73s/it]











 46%|███████████████████████████████████████▋                                              | 970/2100 [2:35:08<3:03:08,  9.72s/it]
