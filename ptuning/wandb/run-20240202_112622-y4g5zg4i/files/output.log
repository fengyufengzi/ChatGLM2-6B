
02/02/2024 11:26:28 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/datasets/load.py:2483: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
[INFO|configuration_utils.py:667] 2024-02-02 11:26:31,867 >> loading configuration file /data/ai/model/chatglm2-6b/config.json
[INFO|configuration_utils.py:667] 2024-02-02 11:26:31,877 >> loading configuration file /data/ai/model/chatglm2-6b/config.json
[INFO|configuration_utils.py:725] 2024-02-02 11:26:31,879 >> Model config ChatGLMConfig {
  "_name_or_path": "/data/ai/model/chatglm2-6b",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1e-05,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_layers": 28,
  "original_rope": true,
  "pad_token_id": 0,
  "padded_vocab_size": 65024,
  "post_layer_norm": true,
  "pre_seq_len": null,
  "prefix_projection": false,
  "quantization_bit": 0,
  "rmsnorm": true,
  "seq_length": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.30.2",
  "use_cache": true,
  "vocab_size": 65024
}
[INFO|tokenization_utils_base.py:1821] 2024-02-02 11:26:31,882 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1821] 2024-02-02 11:26:31,882 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1821] 2024-02-02 11:26:31,883 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1821] 2024-02-02 11:26:31,883 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:2575] 2024-02-02 11:26:31,990 >> loading weights file /data/ai/model/chatglm2-6b/pytorch_model.bin.index.json
[INFO|configuration_utils.py:577] 2024-02-02 11:26:31,990 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}


Loading checkpoint shards:  57%|██████████████████████████████████████▊                             | 4/7 [00:06<00:03,  1.25s/it]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████| 7/7 [00:08<00:00,  1.24s/it]
[INFO|modeling_utils.py:3295] 2024-02-02 11:26:40,726 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.
[WARNING|modeling_utils.py:3297] 2024-02-02 11:26:40,726 >> Some weights of ChatGLMForConditionalGeneration were not initialized from the model checkpoint at /data/ai/model/chatglm2-6b and are newly initialized: ['transformer.prefix_encoder.embedding.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|modeling_utils.py:2927] 2024-02-02 11:26:40,727 >> Generation config file not found, using a generation config created from the model config.
prefix:
inputs什么是蚂蚁庄园
input_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 64790, 64792, 30910, 35318, 41069, 46980]
inputs 什么是蚂蚁庄园
label_ids [64790, 64792, 30910, 41069, 46980, 41680, 33053, 39063, 32488, 39330, 38346, 35439, 32033, 31155]
labels 蚂蚁庄园是一款基于支付宝支付行为的农场养成游戏。
02/02/2024 11:26:44 - INFO - __main__ - *** Evaluate ***
Running tokenizer on validation dataset: 100%|█████████████████████████████████████████| 315/315 [00:00<00:00, 8389.67 examples/s]
[INFO|trainer.py:577] 2024-02-02 11:26:44,166 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:3200] 2024-02-02 11:26:44,169 >> ***** Running Evaluation *****
[INFO|trainer.py:3202] 2024-02-02 11:26:44,169 >>   Num examples = 315
[INFO|trainer.py:3205] 2024-02-02 11:26:44,169 >>   Batch size = 4
[INFO|configuration_utils.py:577] 2024-02-02 11:26:44,171 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
  0%|                                                                                                      | 0/79 [00:00<?, ?it/s][INFO|configuration_utils.py:577] 2024-02-02 11:26:52,057 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
  3%|██▍                                                                                           | 2/79 [00:13<08:23,  6.54s/it][INFO|configuration_utils.py:577] 2024-02-02 11:27:05,132 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
  4%|███▌                                                                                          | 3/79 [00:27<12:12,  9.64s/it][INFO|configuration_utils.py:577] 2024-02-02 11:27:19,112 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
  5%|████▊                                                                                         | 4/79 [00:48<17:30, 14.01s/it][INFO|configuration_utils.py:577] 2024-02-02 11:27:40,474 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
  6%|█████▉                                                                                        | 5/79 [00:55<14:25, 11.70s/it][INFO|configuration_utils.py:577] 2024-02-02 11:27:47,841 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
  8%|███████▏                                                                                      | 6/79 [01:17<18:08, 14.91s/it][INFO|configuration_utils.py:577] 2024-02-02 11:28:09,202 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
  9%|████████▎                                                                                     | 7/79 [01:38<20:22, 16.98s/it][INFO|configuration_utils.py:577] 2024-02-02 11:28:30,562 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 10%|█████████▌                                                                                    | 8/79 [01:59<21:43, 18.36s/it][INFO|configuration_utils.py:577] 2024-02-02 11:28:51,928 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 11%|██████████▋                                                                                   | 9/79 [02:10<18:41, 16.02s/it][INFO|configuration_utils.py:577] 2024-02-02 11:29:02,739 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 13%|███████████▊                                                                                 | 10/79 [02:26<18:21, 15.96s/it][INFO|configuration_utils.py:577] 2024-02-02 11:29:18,568 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 14%|████████████▉                                                                                | 11/79 [02:47<19:57, 17.61s/it][INFO|configuration_utils.py:577] 2024-02-02 11:29:39,935 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 15%|██████████████▏                                                                              | 12/79 [03:09<20:56, 18.75s/it][INFO|configuration_utils.py:577] 2024-02-02 11:30:01,300 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 16%|███████████████▎                                                                             | 13/79 [03:15<16:22, 14.89s/it][INFO|configuration_utils.py:577] 2024-02-02 11:30:07,290 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 18%|████████████████▍                                                                            | 14/79 [03:36<18:14, 16.84s/it][INFO|configuration_utils.py:577] 2024-02-02 11:30:28,655 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 19%|█████████████████▋                                                                           | 15/79 [03:42<14:27, 13.56s/it][INFO|configuration_utils.py:577] 2024-02-02 11:30:34,588 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 20%|██████████████████▊                                                                          | 16/79 [04:01<15:50, 15.09s/it][INFO|configuration_utils.py:577] 2024-02-02 11:30:53,223 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 22%|████████████████████                                                                         | 17/79 [04:22<17:32, 16.97s/it][INFO|configuration_utils.py:577] 2024-02-02 11:31:14,594 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 23%|█████████████████████▏                                                                       | 18/79 [04:43<18:35, 18.29s/it][INFO|configuration_utils.py:577] 2024-02-02 11:31:35,963 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 24%|██████████████████████▎                                                                      | 19/79 [04:52<15:23, 15.39s/it][INFO|configuration_utils.py:577] 2024-02-02 11:31:44,572 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 25%|███████████████████████▌                                                                     | 20/79 [05:07<15:02, 15.30s/it][INFO|configuration_utils.py:577] 2024-02-02 11:31:59,687 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 27%|████████████████████████▋                                                                    | 21/79 [05:28<16:33, 17.12s/it][INFO|configuration_utils.py:577] 2024-02-02 11:32:21,054 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 28%|█████████████████████████▉                                                                   | 22/79 [05:43<15:32, 16.36s/it][INFO|configuration_utils.py:577] 2024-02-02 11:32:35,624 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 29%|███████████████████████████                                                                  | 23/79 [06:04<16:40, 17.86s/it][INFO|configuration_utils.py:577] 2024-02-02 11:32:56,988 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 30%|████████████████████████████▎                                                                | 24/79 [06:15<14:20, 15.64s/it][INFO|configuration_utils.py:577] 2024-02-02 11:33:07,444 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 32%|█████████████████████████████▍                                                               | 25/79 [06:36<15:37, 17.36s/it][INFO|configuration_utils.py:577] 2024-02-02 11:33:28,809 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 33%|██████████████████████████████▌                                                              | 26/79 [06:58<16:23, 18.56s/it][INFO|configuration_utils.py:577] 2024-02-02 11:33:50,179 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 34%|███████████████████████████████▊                                                             | 27/79 [07:19<16:48, 19.40s/it][INFO|configuration_utils.py:577] 2024-02-02 11:34:11,543 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 35%|████████████████████████████████▉                                                            | 28/79 [07:40<16:59, 19.99s/it][INFO|configuration_utils.py:577] 2024-02-02 11:34:32,911 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 37%|██████████████████████████████████▏                                                          | 29/79 [07:58<16:01, 19.23s/it][INFO|configuration_utils.py:577] 2024-02-02 11:34:50,356 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 38%|███████████████████████████████████▎                                                         | 30/79 [08:12<14:22, 17.60s/it][INFO|configuration_utils.py:577] 2024-02-02 11:35:04,152 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 39%|████████████████████████████████████▍                                                        | 31/79 [08:32<14:50, 18.55s/it][INFO|configuration_utils.py:577] 2024-02-02 11:35:24,918 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 41%|█████████████████████████████████████▋                                                       | 32/79 [08:43<12:33, 16.03s/it][INFO|configuration_utils.py:577] 2024-02-02 11:35:35,075 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 42%|██████████████████████████████████████▊                                                      | 33/79 [08:49<09:58, 13.02s/it][INFO|configuration_utils.py:577] 2024-02-02 11:35:41,066 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 43%|████████████████████████████████████████                                                     | 34/79 [08:53<07:50, 10.47s/it][INFO|configuration_utils.py:577] 2024-02-02 11:35:45,573 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 44%|█████████████████████████████████████████▏                                                   | 35/79 [09:14<10:04, 13.74s/it][INFO|configuration_utils.py:577] 2024-02-02 11:36:06,944 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 46%|██████████████████████████████████████████▍                                                  | 36/79 [09:36<11:29, 16.02s/it][INFO|configuration_utils.py:577] 2024-02-02 11:36:28,307 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 47%|███████████████████████████████████████████▌                                                 | 37/79 [09:50<10:53, 15.55s/it][INFO|configuration_utils.py:577] 2024-02-02 11:36:42,756 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 48%|████████████████████████████████████████████▋                                                | 38/79 [10:12<11:49, 17.29s/it][INFO|configuration_utils.py:577] 2024-02-02 11:37:04,117 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 49%|█████████████████████████████████████████████▉                                               | 39/79 [10:33<12:20, 18.51s/it][INFO|configuration_utils.py:577] 2024-02-02 11:37:25,479 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 51%|███████████████████████████████████████████████                                              | 40/79 [10:54<12:35, 19.37s/it][INFO|configuration_utils.py:577] 2024-02-02 11:37:46,843 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 52%|████████████████████████████████████████████████▎                                            | 41/79 [11:16<12:38, 19.97s/it][INFO|configuration_utils.py:577] 2024-02-02 11:38:08,221 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 53%|█████████████████████████████████████████████████▍                                           | 42/79 [11:37<12:34, 20.39s/it][INFO|configuration_utils.py:577] 2024-02-02 11:38:29,590 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 54%|██████████████████████████████████████████████████▌                                          | 43/79 [11:45<09:58, 16.63s/it][INFO|configuration_utils.py:577] 2024-02-02 11:38:37,427 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 56%|███████████████████████████████████████████████████▊                                         | 44/79 [12:06<10:31, 18.05s/it][INFO|configuration_utils.py:577] 2024-02-02 11:38:58,791 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 57%|████████████████████████████████████████████████████▉                                        | 45/79 [12:18<09:08, 16.13s/it][INFO|configuration_utils.py:577] 2024-02-02 11:39:10,438 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 58%|██████████████████████████████████████████████████████▏                                      | 46/79 [12:39<09:44, 17.70s/it][INFO|configuration_utils.py:577] 2024-02-02 11:39:31,803 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 59%|███████████████████████████████████████████████████████▎                                     | 47/79 [12:48<07:59, 14.97s/it][INFO|configuration_utils.py:577] 2024-02-02 11:39:40,410 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 61%|████████████████████████████████████████████████████████▌                                    | 48/79 [13:09<08:43, 16.89s/it][INFO|configuration_utils.py:577] 2024-02-02 11:40:01,770 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 62%|█████████████████████████████████████████████████████████▋                                   | 49/79 [13:26<08:28, 16.96s/it][INFO|configuration_utils.py:577] 2024-02-02 11:40:18,915 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 63%|██████████████████████████████████████████████████████████▊                                  | 50/79 [13:48<08:50, 18.29s/it][INFO|configuration_utils.py:577] 2024-02-02 11:40:40,282 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 65%|████████████████████████████████████████████████████████████                                 | 51/79 [14:09<08:57, 19.21s/it][INFO|configuration_utils.py:577] 2024-02-02 11:41:01,647 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 66%|█████████████████████████████████████████████████████████████▏                               | 52/79 [14:30<08:56, 19.86s/it][INFO|configuration_utils.py:577] 2024-02-02 11:41:23,015 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 67%|██████████████████████████████████████████████████████████████▍                              | 53/79 [14:45<07:56, 18.33s/it][INFO|configuration_utils.py:577] 2024-02-02 11:41:37,766 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 68%|███████████████████████████████████████████████████████████████▌                             | 54/79 [14:53<06:16, 15.07s/it][INFO|configuration_utils.py:577] 2024-02-02 11:41:45,245 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 70%|████████████████████████████████████████████████████████████████▋                            | 55/79 [15:11<06:22, 15.95s/it][INFO|configuration_utils.py:577] 2024-02-02 11:42:03,229 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 71%|█████████████████████████████████████████████████████████████████▉                           | 56/79 [15:25<05:54, 15.39s/it][INFO|configuration_utils.py:577] 2024-02-02 11:42:17,330 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
 72%|███████████████████████████████████████████████████████████████████                          | 57/79 [15:32<04:42, 12.86s/it][INFO|configuration_utils.py:577] 2024-02-02 11:42:24,274 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}
Traceback (most recent call last):
  File "/data/ai/ChatGLM2-6B/ptuning/main.py", line 480, in <module>
    main()
  File "/data/ai/ChatGLM2-6B/ptuning/main.py", line 413, in main
    metrics = trainer.evaluate(metric_key_prefix="eval", do_sample=True, top_p=0.7, max_length=max_seq_length, temperature=0.95)
  File "/data/ai/ChatGLM2-6B/ptuning/trainer_seq2seq.py", line 79, in evaluate
    return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/transformers/trainer.py", line 3053, in evaluate
    output = eval_loop(
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/transformers/trainer.py", line 3245, in evaluation_loop
    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
  File "/data/ai/ChatGLM2-6B/ptuning/trainer_seq2seq.py", line 203, in prediction_step
    generated_tokens = self.model.generate(**gen_kwargs)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/transformers/generation/utils.py", line 1572, in generate
    return self.sample(
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/transformers/generation/utils.py", line 2655, in sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
KeyboardInterrupt
Traceback (most recent call last):
  File "/data/ai/ChatGLM2-6B/ptuning/main.py", line 480, in <module>
    main()
  File "/data/ai/ChatGLM2-6B/ptuning/main.py", line 413, in main
    metrics = trainer.evaluate(metric_key_prefix="eval", do_sample=True, top_p=0.7, max_length=max_seq_length, temperature=0.95)
  File "/data/ai/ChatGLM2-6B/ptuning/trainer_seq2seq.py", line 79, in evaluate
    return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/transformers/trainer.py", line 3053, in evaluate
    output = eval_loop(
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/transformers/trainer.py", line 3245, in evaluation_loop
    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
  File "/data/ai/ChatGLM2-6B/ptuning/trainer_seq2seq.py", line 203, in prediction_step
    generated_tokens = self.model.generate(**gen_kwargs)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/transformers/generation/utils.py", line 1572, in generate
    return self.sample(
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/transformers/generation/utils.py", line 2655, in sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
KeyboardInterrupt