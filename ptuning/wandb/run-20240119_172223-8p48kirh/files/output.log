/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1786] 2024-01-19 17:22:29,771 >> ***** Running training *****
[INFO|trainer.py:1787] 2024-01-19 17:22:29,771 >>   Num examples = 5,726
[INFO|trainer.py:1788] 2024-01-19 17:22:29,771 >>   Num Epochs = 34
[INFO|trainer.py:1789] 2024-01-19 17:22:29,771 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:1790] 2024-01-19 17:22:29,771 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1791] 2024-01-19 17:22:29,771 >>   Gradient Accumulation steps = 16
[INFO|trainer.py:1792] 2024-01-19 17:22:29,771 >>   Total optimization steps = 3,000
[INFO|trainer.py:1793] 2024-01-19 17:22:29,771 >>   Number of trainable parameters = 688,128
[INFO|integrations.py:727] 2024-01-19 17:22:29,798 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|                                                                                                                                         | 0/3000 [00:00<?, ?it/s]/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
01/19/2024 17:22:29 - WARNING - transformers_modules.chatglm2-6b.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...








  0%|▍                                                                                                                              | 9/3000 [01:04<5:54:14,  7.11s/it]










  1%|▊                                                                                                                             | 19/3000 [02:15<5:52:25,  7.09s/it]










  1%|█▏                                                                                                                            | 29/3000 [03:26<5:51:15,  7.09s/it]










  1%|█▋                                                                                                                            | 39/3000 [04:37<5:50:04,  7.09s/it]










  2%|██                                                                                                                            | 49/3000 [05:48<5:48:52,  7.09s/it]










  2%|██▍                                                                                                                           | 59/3000 [06:59<5:47:43,  7.09s/it]










  2%|██▉                                                                                                                           | 69/3000 [08:10<5:46:32,  7.09s/it]










  3%|███▎                                                                                                                          | 79/3000 [09:21<5:45:20,  7.09s/it]










  3%|███▋                                                                                                                          | 89/3000 [10:32<5:44:11,  7.09s/it]







  3%|████                                                                                                                          | 96/3000 [11:21<5:43:07,  7.09s/it]Error in sys.excepthook:
Traceback (most recent call last):
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/linecache.py", line 46, in getlines
    return updatecache(filename, module_globals)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/linecache.py", line 137, in updatecache
    lines = fp.readlines()
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/codecs.py", line 319, in decode
    def decode(self, input, final=False):
KeyboardInterrupt
Original exception was:
Traceback (most recent call last):
  File "/home/ubuntu/Documents/ai/ChatGLM2-6B/ptuning/main.py", line 444, in <module>
    main()
  File "/home/ubuntu/Documents/ai/ChatGLM2-6B/ptuning/main.py", line 379, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/transformers/trainer.py", line 1645, in train
    return inner_training_loop(
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/transformers/trainer.py", line 1938, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/transformers/trainer.py", line 2759, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/transformers/trainer.py", line 2784, in compute_loss
    outputs = model(**inputs)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/.cache/huggingface/modules/transformers_modules/chatglm2-6b/modeling_chatglm.py", line 931, in forward
    transformer_outputs = self.transformer(
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/.cache/huggingface/modules/transformers_modules/chatglm2-6b/modeling_chatglm.py", line 824, in forward
    hidden_states, presents, all_hidden_states, all_self_attentions = self.encoder(
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/.cache/huggingface/modules/transformers_modules/chatglm2-6b/modeling_chatglm.py", line 625, in forward
    layer_ret = torch.utils.checkpoint.checkpoint(
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/_compile.py", line 24, in inner
    return torch._dynamo.disable(fn, recursive)(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 328, in _fn
    return fn(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 17, in inner
    return fn(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 451, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 230, in forward
    outputs = run_function(*args)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/.cache/huggingface/modules/transformers_modules/chatglm2-6b/modeling_chatglm.py", line 538, in forward
    attention_output, kv_cache = self.self_attention(
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/.cache/huggingface/modules/transformers_modules/chatglm2-6b/modeling_chatglm.py", line 375, in forward
    (query_layer, key_layer, value_layer) = mixed_x_layer.split(
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/_tensor.py", line 849, in split
    def split(self, split_size, dim=0):
KeyboardInterrupt