/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1786] 2024-01-23 13:53:44,649 >> ***** Running training *****
[INFO|trainer.py:1787] 2024-01-23 13:53:44,650 >>   Num examples = 10
[INFO|trainer.py:1788] 2024-01-23 13:53:44,650 >>   Num Epochs = 3,000
[INFO|trainer.py:1789] 2024-01-23 13:53:44,650 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1790] 2024-01-23 13:53:44,650 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:1791] 2024-01-23 13:53:44,650 >>   Gradient Accumulation steps = 16
[INFO|trainer.py:1792] 2024-01-23 13:53:44,650 >>   Total optimization steps = 3,000
[INFO|trainer.py:1793] 2024-01-23 13:53:44,650 >>   Number of trainable parameters = 1,835,008
[INFO|integrations.py:727] 2024-01-23 13:53:44,650 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/3000 [00:00<?, ?it/s]/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
01/23/2024 13:53:44 - WARNING - transformers_modules.chatglm2-6b.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...





  0%|          | 8/3000 [00:14<1:16:17,  1.53s/it]







  1%|          | 18/3000 [00:34<1:30:58,  1.83s/it]Traceback (most recent call last):
  File "/home/ubuntu/Documents/ai/ChatGLM2-6B/ptuning/main.py", line 452, in <module>
    main()
  File "/home/ubuntu/Documents/ai/ChatGLM2-6B/ptuning/main.py", line 375, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/transformers/trainer.py", line 1645, in train
    return inner_training_loop(
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/transformers/trainer.py", line 1938, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/transformers/trainer.py", line 2770, in training_step
    self.accelerator.backward(loss)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/accelerate/accelerator.py", line 1964, in backward
    loss.backward(**kwargs)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/ubuntu/Documents/ai/ChatGLM2-6B/ptuning/main.py", line 452, in <module>
    main()
  File "/home/ubuntu/Documents/ai/ChatGLM2-6B/ptuning/main.py", line 375, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/transformers/trainer.py", line 1645, in train
    return inner_training_loop(
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/transformers/trainer.py", line 1938, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/transformers/trainer.py", line 2770, in training_step
    self.accelerator.backward(loss)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/accelerate/accelerator.py", line 1964, in backward
    loss.backward(**kwargs)
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/home/ubuntu/miniconda3/envs/chatglm2/lib/python3.10/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
